{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "# from calc_uauc import uAUC\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "import os\n",
    "import warnings\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_colwidth', 100)\n",
    "tqdm.pandas(desc='pandas bar')\n",
    "\n",
    "\n",
    "def get_count(df, column, feature):\n",
    "    df['idx'] = range(len(df))\n",
    "    temp = df.groupby(column)['userid'].agg([(feature, 'count')]).reset_index()\n",
    "    df = df.merge(temp)\n",
    "    df = df.sort_values('idx').drop('idx', axis=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_sum(df, column, feature):\n",
    "    df['idx'] = range(len(df))\n",
    "    temp = df.groupby(column)['userid'].agg([(feature, 'sum')]).reset_index()\n",
    "    df = df.merge(temp)\n",
    "    df = df.sort_values('idx').drop('idx', axis=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def reduce_mem(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503.54 Mb, 165.94 Mb (67.05 %)\n"
     ]
    }
   ],
   "source": [
    "#train = reduce_mem(train)\n",
    "test = pd.read_csv('./phm_test.csv', header=None)\n",
    "test.columns = ['time','gun_no','C_Cylinder_force','C_Differential_pressure','W_Friction','W_Maximum_aperture','W_Maximum_electrode_force','W_Start_friction','W_us2','W_Welding_point_count','W_position_count','area','in_Counterbalance_pressure','in_Electrode_force','in_Electrode_position','in_Sheet_thickness',\n",
    "'in_Velocity','in_force_build_up','out_Cap_offset','out_Electrode_force','out_Electrode_position','out_force_build_up']\n",
    "test = reduce_mem(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       688128\n",
       "0       272639\n",
       "E003     68572\n",
       "E011     34803\n",
       "E016      2595\n",
       "E029         1\n",
       "Name: W_Error, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.DataFrame(columns = ['time','gun_no','W_Error','C_Cylinder_force','C_Differential_pressure','W_Friction','W_Maximum_aperture','W_Maximum_electrode_force','W_Start_friction','W_us2','W_Welding_point_count','W_position_count','area','in_Counterbalance_pressure','in_Electrode_force','in_Electrode_position','in_Sheet_thickness',\n",
    "'in_Velocity','in_force_build_up','out_Cap_offset','out_Electrode_force','out_Electrode_position','out_force_build_up'])\n",
    "#train.columns = ['time','gun_no','W_Error','C_Cylinder_force','C_Differential_pressure','W_Friction','W_Maximum_aperture','W_Maximum_electrode_force','W_Start_friction','W_us2','W_Welding_point_count','W_position_count','area','in_Counterbalance_pressure','in_Electrode_force','in_Electrode_position','in_Sheet_thickness',\n",
    "#'in_Velocity','in_force_build_up','out_Cap_offset','out_Electrode_force','out_Electrode_position','out_force_build_up']\n",
    "for i in os.listdir(\"./new_train\"):\n",
    "    temp = pd.read_pickle('./new_train/'+i)\n",
    "    train = pd.concat([train, temp[temp[\"W_Error\"] == \"E016\"]])\n",
    "    \n",
    "#print(train.head())\n",
    "#train = pd.read_pickle('./new_train/04-30.pkl')\n",
    "#train.columns = ['time','gun_no','W_Error','C_Cylinder_force','C_Differential_pressure','W_Friction','W_Maximum_aperture','W_Maximum_electrode_force','W_Start_friction','W_us2','W_Welding_point_count','W_position_count','area','in_Counterbalance_pressure','in_Electrode_force','in_Electrode_position','in_Sheet_thickness',\n",
    "#'in_Velocity','in_force_build_up','out_Cap_offset','out_Electrode_force','out_Electrode_position','out_force_build_up']\n",
    "train_positive = pd.read_pickle('./new_train/05-18.pkl')\n",
    "train = pd.concat([train, train_positive])\n",
    "train['W_Error'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1064143\n",
       "1       2595\n",
       "Name: task1_label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['W_Error'][train['W_Error']==0]='0'\n",
    "# train['task1_label'] = 1\n",
    "train['task1_label'] = train['W_Error'].apply(lambda x:1 if x=='E016' else 0)\n",
    "train['task1_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    609632\n",
       "1      1501\n",
       "Name: task1_label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_columns = ['time', 'gun_no', 'area']\n",
    "train.drop(drop_columns, axis=1, inplace=True)\n",
    "test.drop(drop_columns, axis=1, inplace=True)\n",
    "train = train.drop_duplicates()\n",
    "\n",
    "train['task1_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_lgb = np.zeros(len(train))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "col = [tmp_col for tmp_col in train.columns if tmp_col not in ['W_Error', 'task1_label']]\n",
    "#print(train[col].head())\n",
    "X_train = train[col].values\n",
    "# print(train[col].info())\n",
    "y_train = train['task1_label'].values\n",
    "# prediction = np.zeros(len(train))\n",
    "rus = SMOTEENN(random_state=0)\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "print(len(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[20]\ttraining's auc: 0.999964\tvalid_1's auc: 0.999934\n",
      "[40]\ttraining's auc: 0.99996\tvalid_1's auc: 0.999934\n",
      "[60]\ttraining's auc: 0.999972\tvalid_1's auc: 0.999934\n",
      "[80]\ttraining's auc: 0.999977\tvalid_1's auc: 0.999934\n",
      "[100]\ttraining's auc: 0.999985\tvalid_1's auc: 0.999934\n",
      "[120]\ttraining's auc: 0.999992\tvalid_1's auc: 0.999956\n",
      "[140]\ttraining's auc: 0.999992\tvalid_1's auc: 0.999967\n",
      "[160]\ttraining's auc: 0.999993\tvalid_1's auc: 1\n",
      "[180]\ttraining's auc: 0.999999\tvalid_1's auc: 1\n",
      "[200]\ttraining's auc: 0.999999\tvalid_1's auc: 1\n",
      "[220]\ttraining's auc: 1\tvalid_1's auc: 1\n",
      "[240]\ttraining's auc: 1\tvalid_1's auc: 1\n",
      "Early stopping, best iteration is:\n",
      "[152]\ttraining's auc: 0.999992\tvalid_1's auc: 1\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[20]\ttraining's auc: 0.999968\tvalid_1's auc: 0.999513\n",
      "[40]\ttraining's auc: 0.999974\tvalid_1's auc: 0.999712\n",
      "[60]\ttraining's auc: 0.999981\tvalid_1's auc: 0.999701\n",
      "[80]\ttraining's auc: 0.999983\tvalid_1's auc: 0.999779\n",
      "[100]\ttraining's auc: 0.999986\tvalid_1's auc: 0.999823\n",
      "[120]\ttraining's auc: 0.999993\tvalid_1's auc: 0.999834\n",
      "[140]\ttraining's auc: 0.999994\tvalid_1's auc: 0.99979\n",
      "[160]\ttraining's auc: 0.999998\tvalid_1's auc: 0.99979\n",
      "[180]\ttraining's auc: 0.999999\tvalid_1's auc: 0.999756\n",
      "[200]\ttraining's auc: 0.999999\tvalid_1's auc: 0.999745\n",
      "Early stopping, best iteration is:\n",
      "[110]\ttraining's auc: 0.999992\tvalid_1's auc: 0.999834\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[20]\ttraining's auc: 0.999919\tvalid_1's auc: 0.999989\n",
      "[40]\ttraining's auc: 0.999956\tvalid_1's auc: 0.999989\n",
      "[60]\ttraining's auc: 0.999956\tvalid_1's auc: 1\n",
      "[80]\ttraining's auc: 0.999972\tvalid_1's auc: 1\n",
      "[100]\ttraining's auc: 0.999983\tvalid_1's auc: 1\n",
      "[120]\ttraining's auc: 0.999985\tvalid_1's auc: 1\n",
      "[140]\ttraining's auc: 0.999989\tvalid_1's auc: 1\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttraining's auc: 0.999955\tvalid_1's auc: 1\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[20]\ttraining's auc: 0.999872\tvalid_1's auc: 0.999589\n",
      "[40]\ttraining's auc: 0.999922\tvalid_1's auc: 0.999667\n",
      "[60]\ttraining's auc: 0.999953\tvalid_1's auc: 0.999878\n",
      "[80]\ttraining's auc: 0.999971\tvalid_1's auc: 0.999844\n",
      "[100]\ttraining's auc: 0.999975\tvalid_1's auc: 0.9999\n",
      "[120]\ttraining's auc: 0.99998\tvalid_1's auc: 0.999944\n",
      "[140]\ttraining's auc: 0.99999\tvalid_1's auc: 0.999956\n",
      "[160]\ttraining's auc: 0.999997\tvalid_1's auc: 0.999944\n",
      "[180]\ttraining's auc: 0.999998\tvalid_1's auc: 0.999944\n",
      "[200]\ttraining's auc: 0.999999\tvalid_1's auc: 0.999933\n",
      "[220]\ttraining's auc: 1\tvalid_1's auc: 0.999944\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttraining's auc: 0.999989\tvalid_1's auc: 0.999956\n",
      "fold n°5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[20]\ttraining's auc: 0.999906\tvalid_1's auc: 0.999989\n",
      "[40]\ttraining's auc: 0.999936\tvalid_1's auc: 1\n",
      "[60]\ttraining's auc: 0.999968\tvalid_1's auc: 0.999944\n",
      "[80]\ttraining's auc: 0.99997\tvalid_1's auc: 0.999978\n",
      "[100]\ttraining's auc: 0.999985\tvalid_1's auc: 0.999956\n",
      "[120]\ttraining's auc: 0.999986\tvalid_1's auc: 0.999956\n",
      "Early stopping, best iteration is:\n",
      "[35]\ttraining's auc: 0.999906\tvalid_1's auc: 1\n",
      "feature importance:\n",
      "feature\n",
      "in_Counterbalance_pressure    37445.495551\n",
      "out_Electrode_force           13241.567077\n",
      "out_force_build_up             7419.650955\n",
      "in_force_build_up              4442.348731\n",
      "C_Differential_pressure        4401.168159\n",
      "C_Cylinder_force                554.629601\n",
      "out_Electrode_position          420.653748\n",
      "in_Electrode_force              392.055255\n",
      "W_Friction                      201.049089\n",
      "W_us2                           149.787160\n",
      "in_Sheet_thickness              120.090411\n",
      "W_Maximum_aperture               78.608063\n",
      "out_Cap_offset                   72.082248\n",
      "in_Electrode_position            44.987444\n",
      "W_Welding_point_count            20.205252\n",
      "Name: importance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "param = {'num_leaves': 48,\n",
    "         # 'min_data_in_leaf': 30,\n",
    "         'objective': 'binary',\n",
    "         'learning_rate': 0.02,\n",
    "         # 'max_depth': 9,\n",
    "         # \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         # \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         # \"bagging_seed\": 11,\n",
    "         \"metric\": \"auc\",\n",
    "         # \"lambda_l1\": 0.1,\n",
    "         # \"lambda_l2\": 0.1,\n",
    "         \"verbosity\": -1}\n",
    "\n",
    "def f2_score(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    print(precision, recall)\n",
    "    f2_score = 5 * precision * recall / (4 * precision + recall)\n",
    "    return f2_score\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(X_train[trn_idx], y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train[val_idx], y_train[val_idx])\n",
    "\n",
    "    num_round = 2000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets=[trn_data, val_data], verbose_eval=20,\n",
    "                    early_stopping_rounds=100)\n",
    "    oof_lgb[val_idx] = clf.predict(X_train[val_idx], num_iteration=clf.best_iteration)\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = col\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "# oof_lgb2 += clf.predict(df_remove[col].values, num_iteration=clf.best_iteration)\n",
    "# oof_lgb_final = np.argmax(oof_lgb, axis=1)\n",
    "#     predictions_lgb += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "print('feature importance:')\n",
    "print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534.0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000000 entries, 0 to 2999999\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Dtype\n",
      "---  ------  -----\n",
      " 0   0       int8 \n",
      "dtypes: int8(1)\n",
      "memory usage: 2.9 MB\n"
     ]
    }
   ],
   "source": [
    "predict = clf.predict(test, num_iteration=clf.best_iteration)\n",
    "predict[predict > 0.3] = 1\n",
    "predict[predict < 0.3] = 0\n",
    "print(sum(predict))\n",
    "result = pd.DataFrame(predict, dtype='int8')\n",
    "result.info()\n",
    "result.to_csv(\"./predict_e016.csv\", header=0, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[09:48:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.95586\teval-auc:0.95602\n",
      "[200]\ttrain-auc:0.99991\teval-auc:0.99978\n",
      "[400]\ttrain-auc:1.00000\teval-auc:0.99994\n",
      "[544]\ttrain-auc:1.00002\teval-auc:0.99992\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "predict() got an unexpected keyword argument 'num_iteration'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-763c4df9e615>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mwatchlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrn_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'eval'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrn_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwatchlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0moof_lgb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mfold_importance_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mfold_importance_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"feature\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: predict() got an unexpected keyword argument 'num_iteration'"
     ]
    }
   ],
   "source": [
    "params = {'booster': 'gbtree',\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'gamma': 1,\n",
    "            'min_child_weight': 1.5,\n",
    "            'max_depth': 5,\n",
    "            'lambda': 10,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'colsample_bylevel': 0.7,\n",
    "            'eta': 0.04,\n",
    "            'tree_method': 'exact',\n",
    "            'seed': 2020,\n",
    "            'nthread': 36,\n",
    "            \"silent\": True,\n",
    "                      }\n",
    "\n",
    "\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_ + 1))\n",
    "    trn_data = xgb.DMatrix(X_train[trn_idx] , label=y_train[trn_idx])\n",
    "    val_data = xgb.DMatrix(X_train[val_idx] , label=y_train[val_idx])\n",
    "\n",
    "    num_round = 2000\n",
    "    watchlist = [(trn_data, 'train'),(val_data, 'eval')]\n",
    "    clf = xgb.train(params, trn_data, num_boost_round=50000, evals=watchlist, verbose_eval=200, early_stopping_rounds=200)\n",
    "    oof_lgb[val_idx] = clf.predict(X_train[val_idx], num_iteration=clf.best_iteration)\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = col\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "# oof_lgb2 += clf.predict(df_remove[col].values, num_iteration=clf.best_iteration)\n",
    "# oof_lgb_final = np.argmax(oof_lgb, axis=1)\n",
    "#     predictions_lgb += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "print('feature importance:')\n",
    "print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
